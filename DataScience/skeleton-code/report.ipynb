{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3 Data Science Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label I chose for detection was the ship label, this was due to it being in a good position where in most cases it would be surrounded by water to the bottom and the sky to its top, which I belive should make the model easier to train and recognize. This can be seen in sample of the ship images, such as in the make-model notebook, where I first noticed this and chose it above other options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Shape I chose for my Model was a 3 tiered model, this was broken down into 2 inputs and 1 output.\n",
    "The optimal number of units in the first densely-connected layer is 480, this was found during the hyperparameter search.\n",
    "The optimizer was chosen due to it being selected as the best fit in the hyperparameter search, where it was found to be  0.0001.\n",
    "categorical_crossentropy Was chosen for the loss as the dataset had been converted into binary, Either ship present or not, from the original 10 labels, and was used as it provided good results. Additionally other loss options would not change the accuracy or val_loss during epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy (Training and validation) - Train=Orange Validation=Blue\n",
    "![image info](./rptimg/epoch_accuracy.svg)\n",
    "## Loss (Training and Validation)\n",
    "![image info](./rptimg/epoch_loss.svg)\n",
    "## Evalutation of Accuracy\n",
    "![image info](./rptimg/evaluation_accuracy_vs_iterations.svg)\n",
    "## Evaluation of Loss\n",
    "![image info](./rptimg/evaluation_loss_vs_iterations.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Model has trained well, this was expected as it was an improvement of a previous model that was not tuned with hyperparameterisation, yet still had high accuracy.\n",
    "The accuracy within the training reaches very high, with the validation trailing below it, as it starts to seperate its path but is still close to it.\n",
    "The loss also acts as expected, with it reducing itself greatly within the train set, however there is a noticable amount of loss within the validation set, which could be due to sampling techniques.\n",
    "\n",
    "The accuracy also increases with iterations, along with loss decreasing with iterations, which means that the model is good, but definitely could work with more hyperparamters to tune or additionally more layers in the model itself.\n",
    "The evaluation capabilities of the model have been tested, and it works as well as the accuracy describes, recognizing most ship images as such. \n",
    "There is a evaluate.py script which takes in images(only tested on png and jpg) and evaluates what is in them. This also works as well as the model itself and any improvements would need to be made on the model level going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model trained upon the Cifar-10 dataset works well, much better than I had anticipated. I believe the model's accuracy is high due to both the dataset being high quality, and the use of hyperparameters to tune the model and retrain when nessecary.\n",
    "I have not explored the other labels, as the ships label seemed to be good enough, as the model it created has a low Loss of 0.2 and a high accuracy of 92.1%.\n",
    "Further improvement could be made by increasing the layers in the model while still keeping the correct format of the numpy arrays and by more consistent tuning, as I became aware that a large chunk of the models I had tried and tested were not getting tuned as the tuning trial folders were detected and additional tuning was not taking place. This was remedied going forward, to ensure that when the model changed or hyperparameters, the tuning would do a check everytime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. [Learning Multiple Layers of Features from Tiny Images](http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf), Alex Krizhevsky, 2009."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7c0e843705531c78b23726531389a52cc758377181b90b36c808180fa922408"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
